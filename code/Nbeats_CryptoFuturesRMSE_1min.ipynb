{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons keras-beats joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 19:21:58.839164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from kerasbeats import prep_time_series, NBeatsModel\n",
    "from tensorflow import keras\n",
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tscv\n",
    "\n",
    "def generate_label(df, threshold = 0.002):\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['target_15m'] <= -1*threshold), 'label'] = 1\n",
    "    df.loc[(df['target_15m'] >= threshold), 'label'] = 2\n",
    "    return df\n",
    "\n",
    "def get_na_features(df, train_features):\n",
    "    tmp = pd.DataFrame(df[train_features].isnull().sum())\n",
    "    tmp = tmp[tmp[0] > 0].reset_index()\n",
    "    tmp.columns = ['feat', 'cnt']\n",
    "    tmp = tmp.sort_values('cnt')\n",
    "    feat_groups = dict(tmp.groupby('cnt')['feat'].agg(lambda x: list(x)))\n",
    "    return feat_groups\n",
    "\n",
    "def normalize_float_columns(df, features):\n",
    "  float_cols = df[features].select_dtypes(include = [float]).columns\n",
    "  means = df[float_cols].mean().astype('float32')\n",
    "  std = df[float_cols].std().astype('float32')\n",
    "  df[float_cols] = df[float_cols].ffill().fillna(means)\n",
    "  df[float_cols] = (df[float_cols] - means) / std\n",
    "  return df, means, std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "window = 30\n",
    "file = f'mean_corr.csv'\n",
    "corr = pd.read_csv(f'../output/feature_corr_1m/{file}', header = 0, index_col = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = pd.read_feather('../data/df_btc_with_features_1m_spot.feather')\n",
    "\n",
    "df['target'] = df['close'].pct_change(1)\n",
    "df = df.dropna(subset = ['target'], axis = 0)\n",
    "start_time = df['open_time'].min()\n",
    "end_time = df['open_time'].max()\n",
    "dates = df['open_time'].unique()\n",
    "n = len(dates)\n",
    "train_idx = int(0.7 * n)\n",
    "valid_idx = int(0.9 * n)\n",
    "train_end = dates[train_idx]\n",
    "valid_end = dates[valid_idx]\n",
    "\n",
    "train_df = df.loc[df['open_time'] < train_end].reset_index(drop=True)\n",
    "valid_df = df.loc[(train_end <= df['open_time']) & (df['open_time'] < valid_end)].reset_index(drop=True)\n",
    "\n",
    "train_df = pd.concat([train_df, valid_df], axis = 0)\n",
    "\n",
    "test_df = df.loc[(df['open_time'] >= valid_end)].reset_index(drop=True)\n",
    "valid_df = test_df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "groups = pd.factorize(\n",
    "    train_df['open_time'].dt.day.astype(str) + '_' + train_df['open_time'].dt.month.astype(str) + '_' + train_df[\n",
    "        'open_time'].dt.year.astype(str))[0]\n",
    "\n",
    "cv = tscv.PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    group_gap=31,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def create_nbeat_mlp(num_columns, num_labels, lookback, horizon, hidden_units, dropout_rates, batch_size, ls=1e-2, lr=1e-3, ):\n",
    "    nbeats = NBeatsModel(model_type = 'generic', lookback = lookback, horizon = horizon,\n",
    "                         learning_rate = lr, batch_size = batch_size,\n",
    "                         num_generic_neurons = hidden_units[0]) # set as default\n",
    "    nbeats.build_layer()\n",
    "    time_input = keras.layers.Input(shape = (lookback * horizon, ))\n",
    "    x_nb = nbeats.model_layer(time_input)\n",
    "\n",
    "    xcons = keras.layers.Input(shape = (num_columns, ))\n",
    "    x = keras.layers.Concatenate()([xcons, x_nb])\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "    for i in range(1, len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i])(x)\n",
    "\n",
    "    out = tf.keras.layers.Dense(num_labels, name = 'action')(x)\n",
    "    model = tf.keras.models.Model(inputs = [time_input, xcons], outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss = {'action' : tf.keras.losses.Huber(name = 'huber')},\n",
    "                  metrics = {'action' : tf.keras.metrics.R2Score(name = 'mse')})\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "directory = 'spot_data_1130_nbeats_1m'\n",
    "date = '11_27'\n",
    "lib = Path(f\"../output/{directory}\").mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_features = corr.iloc[:50].index.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, means, std = normalize_float_columns(train_df, train_features)\n",
    "\n",
    "nan_features = get_na_features(train_df, train_features)\n",
    "for k, v in nan_features.items():\n",
    "  for feat in v:\n",
    "    train_features.remove(feat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from joblib import load, dump\n",
    "lookback = 3\n",
    "horizon = 500\n",
    "\n",
    "train_features_test = train_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dump(means, f'../output/{directory}/means_nbeats_huber_l={lookback}_h={horizon}_50feats.joblib')\n",
    "dump(std, f'../output/{directory}/std_return_nbeats_l={lookback}_h={horizon}_50feats.joblib')\n",
    "dump(train_features_test, f'../output/{directory}/train_features_test_return_nbeats_l={lookback}_h={horizon}_50feats.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "lookback = 2\n",
    "horizon = 100\n",
    "train_features_test += ['target']\n",
    "params = {'num_columns': len(train_features_test),\n",
    "          'num_labels': 100,\n",
    "          'lookback' : lookback,\n",
    "          'horizon' : horizon,\n",
    "          'batch_size' : batch_size,\n",
    "          'hidden_units': [200, 200, 300, 256],\n",
    "          'dropout_rates': [0.6, 0.5,\n",
    "                            0.6, 0.5],\n",
    "          'ls': 0,\n",
    "          'lr': 1e-3,\n",
    "          }\n",
    "# train_df[train_features_test] = train_df[train_features_test].ffill().fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(train_df, train_df[f'target'], groups)):\n",
    "    if fold >= 4:\n",
    "        x_train, x_valid = train_df['target'].iloc[train_idx], train_df['target'].iloc[val_idx]\n",
    "\n",
    "        min_train, max_train = min(train_df['open_time'].iloc[train_idx]).to_pydatetime(), max(\n",
    "                    train_df['open_time'].iloc[train_idx]).to_pydatetime()\n",
    "        min_valid, max_valid = min(train_df['open_time'].iloc[val_idx]).to_pydatetime(), max(\n",
    "                    train_df['open_time'].iloc[val_idx]).to_pydatetime()\n",
    "\n",
    "\n",
    "        print(f'{fold} : Train Date is from {min_train} - {max_train}')\n",
    "        print(f'{fold} : Valid Date is from {min_valid} - {max_valid}')\n",
    "\n",
    "        x_tr, y_tr = prep_time_series(x_train, lookback = lookback, horizon = horizon)\n",
    "        x_val, y_val = prep_time_series(x_valid, lookback = lookback, horizon = horizon)\n",
    "\n",
    "        cutoff_tr, cutoff_val = x_train.shape[0] - x_tr.shape[0], x_valid.shape[0] - x_val.shape[0]\n",
    "        del x_train, x_valid\n",
    "        gc.collect()\n",
    "\n",
    "        # x_tr_const, x_val_const = train_df[train_features_test].iloc[train_idx], train_df[train_features_test].iloc[val_idx]\n",
    "        # x_tr_const, x_val_const = x_tr_const.iloc[(lookback * horizon) - 1:-horizon, :], x_val_const.iloc[(lookback * horizon) - 1:-horizon, :]\n",
    "\n",
    "        # print(f'Shape of X_const is {x_tr_const.shape}, x_tr is {x_tr.shape}, y_tr is {y_tr.shape}')\n",
    "\n",
    "        # ckp_path = f'../output/{directory}/NBEATS_HUBER_{fold}_returns{horizon}m_{lookback}m_{date}_allfeats_highdropout.hdf5'\n",
    "        # model = create_nbeat_mlp(**params)\n",
    "        # ckp = ModelCheckpoint(ckp_path, monitor='val_r2score', verbose=0,\n",
    "        #                             save_best_only=True, save_weights_only=True, mode='max')\n",
    "        # es = EarlyStopping(monitor='val_r2score', min_delta=1e-4, patience=10, mode='max',\n",
    "        #                     baseline=None, restore_best_weights=True, verbose=0)\n",
    "\n",
    "\n",
    "        # history = model.fit([x_tr, x_tr_const.values], y_tr,\n",
    "        #                     validation_data = ([x_val, x_val_const.values], y_val),\n",
    "        #                     epochs = 100, batch_size = batch_size, callbacks = [ckp, es])\n",
    "\n",
    "        # hist = pd.DataFrame(history.history)\n",
    "        # score = hist['val_r2score'].max()\n",
    "        # print(f'Fold {fold} R2:\\t', score)\n",
    "        # scores.append(score)\n",
    "\n",
    "        # del x_tr, y_tr, x_val, y_val\n",
    "        # gc.collect()\n",
    "        # K.clear_session()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# custom tensorflow metrics\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weighted_mse(true, pred, weights):\n",
    "    sum_weights = tf.reduce_sum(weights)\n",
    "    resid = tf.sqrt(tf.reduce_sum(weights * tf.square(true - pred)))\n",
    "    return resid / sum_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend\n",
    "\n",
    "def custom_mse(class_weights):\n",
    "    def weighted_mse(gt, pred):\n",
    "        # Formula:\n",
    "        # w_1*(y_1-y'_1)^2 + ... + w_100*(y_100-y'_100)^2 / sum(weights)\n",
    "        return backend.sum(class_weights * backend.square(gt - pred)) / backend.sum(class_weights)\n",
    "    return weighted_mse\n",
    "\n",
    "model.compile(loss=custom_mse(weights))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.zeros((2427199))\n",
    "weights[::5] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "weights = np.zeros()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}