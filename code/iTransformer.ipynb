{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b6a6a9-a034-4a4c-9a1d-7d7b83b87709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops==0.4.0 (from -r requirements.txt (line 1))\n",
      "  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting matplotlib==3.7.0 (from -r requirements.txt (line 2))\n",
      "  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.23.5 (from -r requirements.txt (line 3))\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.5.3 (from -r requirements.txt (line 4))\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting patool==1.12 (from -r requirements.txt (line 5))\n",
      "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting reformer-pytorch==1.4.4 (from -r requirements.txt (line 6))\n",
      "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
      "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 7))\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting scipy==1.10.1 (from -r requirements.txt (line 8))\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sktime==0.16.1 (from -r requirements.txt (line 9))\n",
      "  Downloading sktime-0.16.1-py3-none-any.whl (16.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.11.1 (from -r requirements.txt (line 10))\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm==4.64.1 (from -r requirements.txt (line 11))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib==3.7.0->-r requirements.txt (line 2))\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.7.0->-r requirements.txt (line 2))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.7.0->-r requirements.txt (line 2))\n",
      "  Downloading fonttools-4.46.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.2/156.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib==3.7.0->-r requirements.txt (line 2))\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 2)) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas==1.5.3->-r requirements.txt (line 4))\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch==1.4.4->-r requirements.txt (line 6))\n",
      "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting local-attention (from reformer-pytorch==1.4.4->-r requirements.txt (line 6))\n",
      "  Downloading local_attention-1.9.0-py3-none-any.whl.metadata (682 bytes)\n",
      "Collecting product-key-memory (from reformer-pytorch==1.4.4->-r requirements.txt (line 6))\n",
      "  Downloading product_key_memory-0.2.10-py3-none-any.whl.metadata (717 bytes)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (2.1.0+cu118)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.2.2->-r requirements.txt (line 7))\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2->-r requirements.txt (line 7))\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting deprecated>=1.2.13 (from sktime==0.16.1->-r requirements.txt (line 9))\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting numba>=0.53 (from sktime==0.16.1->-r requirements.txt (line 9))\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy==1.11.1->-r requirements.txt (line 10)) (1.3.0)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.13->sktime==0.16.1->-r requirements.txt (line 9))\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba>=0.53->sktime==0.16.1->-r requirements.txt (line 9))\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of local-attention to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting local-attention (from reformer-pytorch==1.4.4->-r requirements.txt (line 6))\n",
      "  Downloading local_attention-1.8.6-py3-none-any.whl (8.1 kB)\n",
      "  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n",
      "  Downloading local_attention-1.8.4-py3-none-any.whl (8.0 kB)\n",
      "  Downloading local_attention-1.8.2-py3-none-any.whl (7.9 kB)\n",
      "  Downloading local_attention-1.8.1-py3-none-any.whl (8.0 kB)\n",
      "  Downloading local_attention-1.8.0-py3-none-any.whl (7.9 kB)\n",
      "  Downloading local_attention-1.7.2-py3-none-any.whl (7.5 kB)\n",
      "INFO: pip is still looking at multiple versions of local-attention to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading local_attention-1.7.1-py3-none-any.whl (7.4 kB)\n",
      "  Downloading local_attention-1.7.0-py3-none-any.whl (7.4 kB)\n",
      "  Downloading local_attention-1.6.0-py3-none-any.whl (7.0 kB)\n",
      "  Downloading local_attention-1.5.8-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.7-py3-none-any.whl (6.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading local_attention-1.5.6-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.5-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.4-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.3-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.2-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.1-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.0-py3-none-any.whl (6.7 kB)\n",
      "  Downloading local_attention-1.4.4-py3-none-any.whl (5.0 kB)\n",
      "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer-pytorch==1.4.4->-r requirements.txt (line 6))\n",
      "  Downloading CoLT5_attention-0.10.19-py3-none-any.whl.metadata (738 bytes)\n",
      "INFO: pip is looking at multiple versions of product-key-memory to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting product-key-memory (from reformer-pytorch==1.4.4->-r requirements.txt (line 6))\n",
      "  Downloading product_key_memory-0.2.9-py3-none-any.whl.metadata (720 bytes)\n",
      "  Downloading product_key_memory-0.2.8-py3-none-any.whl.metadata (720 bytes)\n",
      "  Downloading product_key_memory-0.2.7-py3-none-any.whl.metadata (720 bytes)\n",
      "  Downloading product_key_memory-0.2.6-py3-none-any.whl.metadata (720 bytes)\n",
      "  Downloading product_key_memory-0.2.5-py3-none-any.whl.metadata (720 bytes)\n",
      "  Downloading product_key_memory-0.2.4-py3-none-any.whl.metadata (720 bytes)\n",
      "  Downloading product_key_memory-0.2.3-py3-none-any.whl.metadata (720 bytes)\n",
      "INFO: pip is still looking at multiple versions of product-key-memory to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading product_key_memory-0.2.2-py3-none-any.whl.metadata (677 bytes)\n",
      "  Downloading product_key_memory-0.2.1-py3-none-any.whl.metadata (677 bytes)\n",
      "  Downloading product_key_memory-0.2.0-py3-none-any.whl.metadata (677 bytes)\n",
      "  Downloading product_key_memory-0.1.11-py3-none-any.whl.metadata (678 bytes)\n",
      "  Downloading product_key_memory-0.1.10.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->reformer-pytorch==1.4.4->-r requirements.txt (line 6)) (2.1.2)\n",
      "Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading fonttools-4.46.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: axial-positional-embedding, product-key-memory\n",
      "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2882 sha256=885a33c550a9dca6023fa11ac0acecc4a2dfbf5f456e11d57f4751dcb20c075c\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
      "  Building wheel for product-key-memory (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-py3-none-any.whl size=3051 sha256=c86a8fba7730c54714eb8a3d08574a75fec2098e63fe7e93b1b97f37a8bce8a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/97/82/f94ef75772952e1eaec19864fc840620ec94d9ac7c9c0b6823\n",
      "Successfully built axial-positional-embedding product-key-memory\n",
      "Installing collected packages: pytz, patool, einops, wrapt, tqdm, threadpoolctl, sympy, numpy, llvmlite, kiwisolver, joblib, fonttools, cycler, scipy, pandas, numba, deprecated, contourpy, scikit-learn, product-key-memory, matplotlib, local-attention, axial-positional-embedding, sktime, reformer-pytorch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed axial-positional-embedding-0.2.1 contourpy-1.2.0 cycler-0.12.1 deprecated-1.2.14 einops-0.4.0 fonttools-4.46.0 joblib-1.3.2 kiwisolver-1.4.5 llvmlite-0.41.1 local-attention-1.4.4 matplotlib-3.7.0 numba-0.58.1 numpy-1.23.5 pandas-1.5.3 patool-1.12 product-key-memory-0.1.10 pytz-2023.3.post1 reformer-pytorch-1.4.4 scikit-learn-1.2.2 scipy-1.10.1 sktime-0.16.1 sympy-1.11.1 threadpoolctl-3.2.0 tqdm-4.64.1 wrapt-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-14.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install pandas joblib pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100f7e3e-a01f-4012-8c53-65520f69cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.Conv_Blocks import Inception_Block_V1   \n",
    "            #convolution block used for convoluting the 2D time data, changeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634d98b6-a283-429c-803b-f15845c0135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='TimesNet')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--task_name', type=str, default='long_term_forecast',\n",
    "                    help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, default='TimesNet',\n",
    "                    help='model name, options: [Autoformer, Transformer, TimesNet]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, default='custom', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='../../data/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='df_btc_with_features_5m_spot_new_features.feather', help='data file')\n",
    "parser.add_argument('--features', type=str, default='MS',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='target', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='5min',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "parser.add_argument('--seasonal_patterns', type=str, default='Monthly', help='subset for M4')\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "\n",
    "# inputation task\n",
    "# parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "\n",
    "# anomaly detection task\n",
    "# parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%)')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--top_k', type=int, default=5, help='for TimesBlock')\n",
    "parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=1024, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=10, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# de-stationary projector params\n",
    "parser.add_argument('--p_hidden_dims', type=int, nargs='+', default=[128, 128],\n",
    "                    help='hidden layer dimensions of projector (List)')\n",
    "parser.add_argument('--p_hidden_layers', type=int, default=2, help='number of hidden layers in projector')\n",
    "\n",
    "\n",
    "config = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5285a516-41d8-4e53-bafc-debc816f763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "\n",
    "    #first type: learning rate decrease with epoch by exponential\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "\n",
    "    #second type: learning rate decrease manually\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            8: 1e-4, 16 : 1e-5\n",
    "        }\n",
    "\n",
    "    #1st type: update in each epoch\n",
    "    #2nd type: only update in epochs that are written in Dict lr_adjust\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "    \n",
    "        # change the learning rate for different parameter groups within the optimizer\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65e889f-3cf6-4962-8002-c757117f59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_factory import data_provider\n",
    "from exp.exp_basic import Exp_Basic\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from utils.metrics import metric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Exp_Long_Term_Forecast(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_Long_Term_Forecast, self).__init__(args)\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = self.model_dict[self.args.model].Model(self.args).float()\n",
    "\n",
    "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
    "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
    "        return model\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = nn.MSELoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if self.args.output_attention:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "                total_loss.append(loss)\n",
    "        total_loss = np.average(total_loss)\n",
    "        self.model.train()\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag='train')\n",
    "        vali_data, vali_loader = self._get_data(flag='val')\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "\n",
    "        path = os.path.join(self.args.checkpoints, setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "\n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion = self._select_criterion()\n",
    "\n",
    "        if self.args.use_amp:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                        f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if self.args.output_attention:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                    f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                    batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                if self.args.use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(model_optim)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    model_optim.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
    "            test_loss = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping(vali_loss, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            adjust_learning_rate(model_optim, epoch + 1, self.args)\n",
    "\n",
    "        best_model_path = path + '/' + 'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if self.args.output_attention:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, :]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, :].to(self.device)\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                batch_y = batch_y.detach().cpu().numpy()\n",
    "                if test_data.scale and self.args.inverse:\n",
    "                    shape = outputs.shape\n",
    "                    outputs = test_data.inverse_transform(outputs.squeeze(0)).reshape(shape)\n",
    "                    batch_y = test_data.inverse_transform(batch_y.squeeze(0)).reshape(shape)\n",
    "        \n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "                batch_y = batch_y[:, :, f_dim:]\n",
    "\n",
    "                pred = outputs\n",
    "                true = batch_y\n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                if i % 20 == 0:\n",
    "                    input = batch_x.detach().cpu().numpy()\n",
    "                    if test_data.scale and self.args.inverse:\n",
    "                        shape = input.shape\n",
    "                        input = test_data.inverse_transform(input.squeeze(0)).reshape(shape)\n",
    "                    gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                    pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        trues = np.array(trues)\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
    "        print('mse:{}, mae:{}'.format(mse, mae))\n",
    "        f = open(\"result_long_term_forecast.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('mse:{}, mae:{}'.format(mse, mae))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "        np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\n",
    "        np.save(folder_path + 'pred.npy', preds)\n",
    "        np.save(folder_path + 'true.npy', trues)\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c2ec17d-7fc0-406b-bcfb-65585c9586ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are some dataloaders defined in data_loader.py. If you want to add your own data, \n",
    "# go and check data_loader.py to rewrite a dataloader to fit your data.\n",
    "data_dict = {\n",
    "    'custom': Dataset_Custom,\n",
    "}\n",
    "\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = data_dict[args.data]  #data_provider\n",
    "\n",
    "    # time features encoding, options:[timeF, fixed, learned]\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "\n",
    "    #test data provider\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        if args.task_name == 'anomaly_detection' or args.task_name == 'classification':\n",
    "            batch_size = args.batch_size\n",
    "\n",
    "        #Some tasks during the testing phase may require evaluating samples one at a time. \n",
    "        # This could be due to variations in sample sizes in the test data or because the \n",
    "        # evaluation process demands finer-grained results or different processing. \n",
    "        else:\n",
    "            batch_size = args.batch_size  # bsz=1 for evaluation\n",
    "\n",
    "        #freq for time features encoding, \n",
    "        # options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly,\n",
    "        #  m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "        freq = args.freq\n",
    "    else:\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size  # bsz for train and valid\n",
    "        freq = args.freq\n",
    "\n",
    "    if args.task_name == 'anomaly_detection':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            # root_path=args.root_path, #root path of the data file\n",
    "            # win_size=args.seq_len,    #input sequence length\n",
    "            flag=flag,\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,#data loader num workers\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    elif args.task_name == 'classification':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            flag=flag,\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last,\n",
    "            collate_fn=lambda x: collate_fn(x, max_len=args.seq_len) \n",
    "            #define some limits to collate pieces of data into batches\n",
    "        )\n",
    "        return data_set, data_loader\n",
    "    else:\n",
    "        if args.data == 'm4':\n",
    "            drop_last = False\n",
    "        data_set = Data(\n",
    "            # root_path=args.root_path, #eg.  ./data/ETT/\n",
    "            # data_path=args.data_path, #eg.  ETTh1.csv\n",
    "            flag=flag,\n",
    "            size=[args.seq_len, args.label_len, args.pred_len],\n",
    "            features=args.features,   #forecasting task, options:[M, S, MS]; \n",
    "            # M:multivariate predict multivariate, S:univariate predict univariate,\n",
    "            # MS:multivariate predict univariate\n",
    "            \n",
    "            target=args.target,       #target feature in S or MS task\n",
    "            timeenc=timeenc,\n",
    "            freq=freq,\n",
    "            seasonal_patterns=args.seasonal_patterns\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a06ee6b1-01b0-4bd1-8809-08465048c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(args, flag):\n",
    "        data_set, data_loader = data_provider(args, flag)\n",
    "        return data_set, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fab27d03-508c-46fb-bee2-823c8ae558bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = '../../output/feature_corr/mean_corr_new_features.csv'\n",
    "corr = pd.read_csv(feature_path, header = 0, index_col = 0)\n",
    "features = corr.iloc[:50].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb1cbb3d-7376-493f-aa22-dcafbd767fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.root_path = '../../data/'\n",
    "config.data_path = 'df_btc_with_features_5m_spot_new_features.feather'\n",
    "config.target = 'target'\n",
    "config.embed = 'timeF'\n",
    "config.features = 'MS'\n",
    "config.seq_len = 300\n",
    "config.label_len = 300\n",
    "config.pred_len = 100\n",
    "config.target = 'target'\n",
    "config.enc_in = len(features) + 1\n",
    "config.dec_in = 20\n",
    "config.c_out = len(features) + 1\n",
    "config.itr = 1\n",
    "config.batch_size = 1024\n",
    "config.moving_avg = 25\n",
    "config.factor = 3\n",
    "config.e_layers = 4\n",
    "config.d_layers = 4\n",
    "config.freq = 't'\n",
    "config.dropout = 0.4\n",
    "config.devices = '0'\n",
    "config.d_ff = 256\n",
    "config.d_model = 512\n",
    "config.model = 'iTransformer'\n",
    "config.p_hidden_dims = [512, 512, 512]\n",
    "config.p_hidden_layers = 3\n",
    "config.seasonal_patterns = 'Hourly'\n",
    "config.lradj = 'type2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5a9d813-0cdd-4a83-8ad2-e88a4cca5169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_term_forecast_test_iTransformer_custom_ftMS_sl300_ll300_pl100_dm512_nh8_el4_dl4_df256_fc3_ebtimeF_dtTrue_test_0\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "    config.task_name,\n",
    "    config.model_id,\n",
    "    config.model,\n",
    "    config.data,\n",
    "    config.features,\n",
    "    config.seq_len,\n",
    "    config.label_len,\n",
    "    config.pred_len,\n",
    "    config.d_model,\n",
    "    config.n_heads,\n",
    "    config.e_layers,\n",
    "    config.d_layers,\n",
    "    config.d_ff,\n",
    "    config.factor,\n",
    "    config.embed,\n",
    "    config.distil,\n",
    "    config.des, ii)\n",
    "\n",
    "print(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a0df17e-f614-43c4-a9d3-a49349eeee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "target\n",
      "train 461248\n",
      "target\n",
      "val 65852\n",
      "target\n",
      "test 131800\n",
      "\titers: 100, epoch: 1 | loss: 0.8190042\n",
      "\tspeed: 0.2676s/iter; left time: 12013.3751s\n",
      "\titers: 200, epoch: 1 | loss: 0.2601871\n",
      "\tspeed: 0.2417s/iter; left time: 10827.3451s\n",
      "\titers: 300, epoch: 1 | loss: 0.0644026\n",
      "\tspeed: 0.2390s/iter; left time: 10683.8824s\n",
      "\titers: 400, epoch: 1 | loss: 0.3921428\n",
      "\tspeed: 0.2382s/iter; left time: 10623.8701s\n",
      "Epoch: 1 cost time: 110.69549345970154\n",
      "Epoch: 1, Steps: 450 | Train Loss: 1.0362189 Vali Loss: 0.5644007 Test Loss: 0.2343007\n",
      "Validation loss decreased (inf --> 0.564401).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.8075547\n",
      "\tspeed: 0.9733s/iter; left time: 43262.9682s\n",
      "\titers: 200, epoch: 2 | loss: 0.2582199\n",
      "\tspeed: 0.2455s/iter; left time: 10889.6494s\n",
      "\titers: 300, epoch: 2 | loss: 0.0639944\n",
      "\tspeed: 0.2382s/iter; left time: 10540.1171s\n",
      "\titers: 400, epoch: 2 | loss: 0.3914276\n",
      "\tspeed: 0.2378s/iter; left time: 10499.6022s\n",
      "Epoch: 2 cost time: 111.24186611175537\n",
      "Epoch: 2, Steps: 450 | Train Loss: 1.0040702 Vali Loss: 0.5642684 Test Loss: 0.2342381\n",
      "Validation loss decreased (0.564401 --> 0.564268).  Saving model ...\n",
      "\titers: 100, epoch: 3 | loss: 0.8070254\n",
      "\tspeed: 0.9710s/iter; left time: 42724.4521s\n",
      "\titers: 200, epoch: 3 | loss: 0.2580465\n",
      "\tspeed: 0.2382s/iter; left time: 10456.7821s\n",
      "\titers: 300, epoch: 3 | loss: 0.0638946\n",
      "\tspeed: 0.2433s/iter; left time: 10658.7684s\n",
      "\titers: 400, epoch: 3 | loss: 0.3908228\n",
      "\tspeed: 0.2487s/iter; left time: 10866.4199s\n",
      "Epoch: 3 cost time: 111.19733738899231\n",
      "Epoch: 3, Steps: 450 | Train Loss: 1.0030955 Vali Loss: 0.5640724 Test Loss: 0.2341882\n",
      "Validation loss decreased (0.564268 --> 0.564072).  Saving model ...\n",
      "\titers: 100, epoch: 4 | loss: 0.8070037\n",
      "\tspeed: 0.9690s/iter; left time: 42200.9314s\n",
      "\titers: 200, epoch: 4 | loss: 0.2580304\n",
      "\tspeed: 0.2357s/iter; left time: 10242.4010s\n",
      "\titers: 300, epoch: 4 | loss: 0.0639184\n",
      "\tspeed: 0.2419s/iter; left time: 10487.3643s\n",
      "\titers: 400, epoch: 4 | loss: 0.3911350\n",
      "\tspeed: 0.2440s/iter; left time: 10553.1464s\n",
      "Epoch: 4 cost time: 109.52805089950562\n",
      "Epoch: 4, Steps: 450 | Train Loss: 1.0026602 Vali Loss: 0.5642148 Test Loss: 0.2342056\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\titers: 100, epoch: 5 | loss: 0.8071045\n",
      "\tspeed: 0.9694s/iter; left time: 41780.6682s\n",
      "\titers: 200, epoch: 5 | loss: 0.2579882\n",
      "\tspeed: 0.2584s/iter; left time: 11111.3310s\n",
      "\titers: 300, epoch: 5 | loss: 0.0638525\n",
      "\tspeed: 0.2683s/iter; left time: 11508.8869s\n",
      "\titers: 400, epoch: 5 | loss: 0.3910357\n",
      "\tspeed: 0.2467s/iter; left time: 10557.8944s\n",
      "Epoch: 5 cost time: 115.77520084381104\n",
      "Epoch: 5, Steps: 450 | Train Loss: 1.0026236 Vali Loss: 0.5642081 Test Loss: 0.2342071\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\titers: 100, epoch: 6 | loss: 0.8067861\n",
      "\tspeed: 0.9787s/iter; left time: 41740.7332s\n",
      "\titers: 200, epoch: 6 | loss: 0.2578796\n",
      "\tspeed: 0.2410s/iter; left time: 10254.6653s\n",
      "\titers: 300, epoch: 6 | loss: 0.0638887\n",
      "\tspeed: 0.2398s/iter; left time: 10180.9989s\n",
      "\titers: 400, epoch: 6 | loss: 0.3910461\n",
      "\tspeed: 0.2430s/iter; left time: 10290.4753s\n",
      "Epoch: 6 cost time: 111.04249405860901\n",
      "Epoch: 6, Steps: 450 | Train Loss: 1.0023733 Vali Loss: 0.5641999 Test Loss: 0.2342012\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\titers: 100, epoch: 7 | loss: 0.8072619\n",
      "\tspeed: 0.9562s/iter; left time: 40352.1708s\n",
      "\titers: 200, epoch: 7 | loss: 0.2580728\n",
      "\tspeed: 0.2400s/iter; left time: 10104.2041s\n",
      "\titers: 300, epoch: 7 | loss: 0.0637915\n",
      "\tspeed: 0.2440s/iter; left time: 10248.1850s\n",
      "\titers: 400, epoch: 7 | loss: 0.3909382\n",
      "\tspeed: 0.2386s/iter; left time: 9996.8944s\n",
      "Epoch: 7 cost time: 110.09253168106079\n",
      "Epoch: 7, Steps: 450 | Train Loss: 1.0023015 Vali Loss: 0.5640209 Test Loss: 0.2341338\n",
      "Validation loss decreased (0.564072 --> 0.564021).  Saving model ...\n",
      "\titers: 100, epoch: 8 | loss: 0.8068616\n",
      "\tspeed: 0.9644s/iter; left time: 40265.4291s\n",
      "\titers: 200, epoch: 8 | loss: 0.2579671\n",
      "\tspeed: 0.2540s/iter; left time: 10579.2789s\n",
      "\titers: 300, epoch: 8 | loss: 0.0638184\n",
      "\tspeed: 0.2377s/iter; left time: 9876.0005s\n",
      "\titers: 400, epoch: 8 | loss: 0.3909239\n",
      "\tspeed: 0.2363s/iter; left time: 9795.5014s\n",
      "Epoch: 8 cost time: 111.06747484207153\n",
      "Epoch: 8, Steps: 450 | Train Loss: 1.0023105 Vali Loss: 0.5640897 Test Loss: 0.2341621\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 9 | loss: 0.8065454\n",
      "\tspeed: 0.9990s/iter; left time: 41259.8203s\n",
      "\titers: 200, epoch: 9 | loss: 0.2579874\n",
      "\tspeed: 0.2408s/iter; left time: 9922.4858s\n",
      "\titers: 300, epoch: 9 | loss: 0.0637460\n",
      "\tspeed: 0.2362s/iter; left time: 9706.6576s\n",
      "\titers: 400, epoch: 9 | loss: 0.3907294\n",
      "\tspeed: 0.2457s/iter; left time: 10072.5480s\n",
      "Epoch: 9 cost time: 110.87914633750916\n",
      "Epoch: 9, Steps: 450 | Train Loss: 1.0018615 Vali Loss: 0.5638521 Test Loss: 0.2340674\n",
      "Validation loss decreased (0.564021 --> 0.563852).  Saving model ...\n",
      "\titers: 100, epoch: 10 | loss: 0.8061885\n",
      "\tspeed: 0.9622s/iter; left time: 39305.6389s\n",
      "\titers: 200, epoch: 10 | loss: 0.2579233\n",
      "\tspeed: 0.2492s/iter; left time: 10153.5877s\n",
      "\titers: 300, epoch: 10 | loss: 0.0637152\n",
      "\tspeed: 0.2386s/iter; left time: 9701.1047s\n",
      "\titers: 400, epoch: 10 | loss: 0.3905154\n",
      "\tspeed: 0.2436s/iter; left time: 9878.2818s\n",
      "Epoch: 10 cost time: 110.81490850448608\n",
      "Epoch: 10, Steps: 450 | Train Loss: 1.0016655 Vali Loss: 0.5637507 Test Loss: 0.2340583\n",
      "Validation loss decreased (0.563852 --> 0.563751).  Saving model ...\n",
      "\titers: 100, epoch: 11 | loss: 0.8063285\n",
      "\tspeed: 0.9718s/iter; left time: 39259.9201s\n",
      "\titers: 200, epoch: 11 | loss: 0.2578906\n",
      "\tspeed: 0.2354s/iter; left time: 9486.5764s\n",
      "\titers: 300, epoch: 11 | loss: 0.0636974\n",
      "\tspeed: 0.2432s/iter; left time: 9778.8545s\n",
      "\titers: 400, epoch: 11 | loss: 0.3904648\n",
      "\tspeed: 0.2392s/iter; left time: 9591.3479s\n",
      "Epoch: 11 cost time: 110.15060448646545\n",
      "Epoch: 11, Steps: 450 | Train Loss: 1.0015339 Vali Loss: 0.5636997 Test Loss: 0.2340578\n",
      "Validation loss decreased (0.563751 --> 0.563700).  Saving model ...\n",
      "\titers: 100, epoch: 12 | loss: 0.8062335\n",
      "\tspeed: 0.9842s/iter; left time: 39318.9763s\n",
      "\titers: 200, epoch: 12 | loss: 0.2578769\n",
      "\tspeed: 0.2456s/iter; left time: 9789.2472s\n",
      "\titers: 300, epoch: 12 | loss: 0.0636659\n",
      "\tspeed: 0.2392s/iter; left time: 9507.0244s\n",
      "\titers: 400, epoch: 12 | loss: 0.3904007\n",
      "\tspeed: 0.2422s/iter; left time: 9602.8471s\n",
      "Epoch: 12 cost time: 111.51151990890503\n",
      "Epoch: 12, Steps: 450 | Train Loss: 1.0014423 Vali Loss: 0.5636814 Test Loss: 0.2340555\n",
      "Validation loss decreased (0.563700 --> 0.563681).  Saving model ...\n",
      "\titers: 100, epoch: 13 | loss: 0.8062682\n",
      "\tspeed: 0.9750s/iter; left time: 38513.5670s\n",
      "\titers: 200, epoch: 13 | loss: 0.2578325\n",
      "\tspeed: 0.2382s/iter; left time: 9385.2531s\n",
      "\titers: 300, epoch: 13 | loss: 0.0636587\n",
      "\tspeed: 0.2400s/iter; left time: 9430.9202s\n",
      "\titers: 400, epoch: 13 | loss: 0.3904418\n",
      "\tspeed: 0.2416s/iter; left time: 9470.7713s\n",
      "Epoch: 13 cost time: 110.5748679637909\n",
      "Epoch: 13, Steps: 450 | Train Loss: 1.0013250 Vali Loss: 0.5636489 Test Loss: 0.2340485\n",
      "Validation loss decreased (0.563681 --> 0.563649).  Saving model ...\n",
      "\titers: 100, epoch: 14 | loss: 0.8064920\n",
      "\tspeed: 0.9822s/iter; left time: 38357.4035s\n",
      "\titers: 200, epoch: 14 | loss: 0.2578445\n",
      "\tspeed: 0.2447s/iter; left time: 9531.7935s\n",
      "\titers: 300, epoch: 14 | loss: 0.0636801\n",
      "\tspeed: 0.2393s/iter; left time: 9296.4961s\n",
      "\titers: 400, epoch: 14 | loss: 0.3903434\n",
      "\tspeed: 0.2430s/iter; left time: 9416.4664s\n",
      "Epoch: 14 cost time: 111.15716934204102\n",
      "Epoch: 14, Steps: 450 | Train Loss: 1.0012452 Vali Loss: 0.5636363 Test Loss: 0.2340425\n",
      "Validation loss decreased (0.563649 --> 0.563636).  Saving model ...\n",
      "\titers: 100, epoch: 15 | loss: 0.8064398\n",
      "\tspeed: 0.9827s/iter; left time: 37933.7769s\n",
      "\titers: 200, epoch: 15 | loss: 0.2579044\n",
      "\tspeed: 0.2470s/iter; left time: 9510.0785s\n",
      "\titers: 300, epoch: 15 | loss: 0.0636492\n",
      "\tspeed: 0.2394s/iter; left time: 9193.7915s\n",
      "\titers: 400, epoch: 15 | loss: 0.3902831\n",
      "\tspeed: 0.2413s/iter; left time: 9240.6377s\n",
      "Epoch: 15 cost time: 111.5073390007019\n",
      "Epoch: 15, Steps: 450 | Train Loss: 1.0011683 Vali Loss: 0.5636175 Test Loss: 0.2340329\n",
      "Validation loss decreased (0.563636 --> 0.563618).  Saving model ...\n",
      "\titers: 100, epoch: 16 | loss: 0.8063304\n",
      "\tspeed: 0.9910s/iter; left time: 37806.1012s\n",
      "\titers: 200, epoch: 16 | loss: 0.2578838\n",
      "\tspeed: 0.2396s/iter; left time: 9118.4546s\n",
      "\titers: 300, epoch: 16 | loss: 0.0636443\n",
      "\tspeed: 0.2408s/iter; left time: 9139.4604s\n",
      "\titers: 400, epoch: 16 | loss: 0.3902272\n",
      "\tspeed: 0.2459s/iter; left time: 9309.2170s\n",
      "Epoch: 16 cost time: 111.55777049064636\n",
      "Epoch: 16, Steps: 450 | Train Loss: 1.0011139 Vali Loss: 0.5636149 Test Loss: 0.2340409\n",
      "Validation loss decreased (0.563618 --> 0.563615).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.8063870\n",
      "\tspeed: 0.9679s/iter; left time: 36492.2411s\n",
      "\titers: 200, epoch: 17 | loss: 0.2578411\n",
      "\tspeed: 0.2380s/iter; left time: 8947.3752s\n",
      "\titers: 300, epoch: 17 | loss: 0.0636487\n",
      "\tspeed: 0.2341s/iter; left time: 8780.3281s\n",
      "\titers: 400, epoch: 17 | loss: 0.3902798\n",
      "\tspeed: 0.2423s/iter; left time: 9063.8973s\n",
      "Epoch: 17 cost time: 109.66979765892029\n",
      "Epoch: 17, Steps: 450 | Train Loss: 1.0010449 Vali Loss: 0.5636188 Test Loss: 0.2340519\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\titers: 100, epoch: 18 | loss: 0.8060408\n",
      "\tspeed: 0.9569s/iter; left time: 35643.9713s\n",
      "\titers: 200, epoch: 18 | loss: 0.2579336\n",
      "\tspeed: 0.2450s/iter; left time: 9101.8603s\n",
      "\titers: 300, epoch: 18 | loss: 0.0636356\n",
      "\tspeed: 0.2390s/iter; left time: 8855.7739s\n",
      "\titers: 400, epoch: 18 | loss: 0.3904714\n",
      "\tspeed: 0.2408s/iter; left time: 8897.6366s\n",
      "Epoch: 18 cost time: 110.89868259429932\n",
      "Epoch: 18, Steps: 450 | Train Loss: 1.0009589 Vali Loss: 0.5636240 Test Loss: 0.2340538\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\titers: 100, epoch: 19 | loss: 0.8059654\n",
      "\tspeed: 0.9532s/iter; left time: 35078.3723s\n",
      "\titers: 200, epoch: 19 | loss: 0.2580350\n",
      "\tspeed: 0.2530s/iter; left time: 9285.2918s\n",
      "\titers: 300, epoch: 19 | loss: 0.0636581\n",
      "\tspeed: 0.2411s/iter; left time: 8825.6831s\n",
      "\titers: 400, epoch: 19 | loss: 0.3905241\n",
      "\tspeed: 0.2379s/iter; left time: 8682.3120s\n",
      "Epoch: 19 cost time: 111.16104793548584\n",
      "Epoch: 19, Steps: 450 | Train Loss: 1.0009379 Vali Loss: 0.5636228 Test Loss: 0.2340545\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\titers: 100, epoch: 20 | loss: 0.8062380\n",
      "\tspeed: 0.9696s/iter; left time: 35245.9634s\n",
      "\titers: 200, epoch: 20 | loss: 0.2579755\n",
      "\tspeed: 0.2432s/iter; left time: 8816.6181s\n",
      "\titers: 300, epoch: 20 | loss: 0.0636539\n",
      "\tspeed: 0.2353s/iter; left time: 8507.3543s\n",
      "\titers: 400, epoch: 20 | loss: 0.3904662\n",
      "\tspeed: 0.2409s/iter; left time: 8683.3060s\n",
      "Epoch: 20 cost time: 109.53576850891113\n",
      "Epoch: 20, Steps: 450 | Train Loss: 1.0008930 Vali Loss: 0.5636247 Test Loss: 0.2340591\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\titers: 100, epoch: 21 | loss: 0.8059804\n",
      "\tspeed: 0.9866s/iter; left time: 35421.0241s\n",
      "\titers: 200, epoch: 21 | loss: 0.2580223\n",
      "\tspeed: 0.2464s/iter; left time: 8820.2296s\n",
      "\titers: 300, epoch: 21 | loss: 0.0636685\n",
      "\tspeed: 0.2408s/iter; left time: 8596.2183s\n",
      "\titers: 400, epoch: 21 | loss: 0.3903395\n",
      "\tspeed: 0.2412s/iter; left time: 8587.5287s\n",
      "Epoch: 21 cost time: 112.23610877990723\n",
      "Epoch: 21, Steps: 450 | Train Loss: 1.0008747 Vali Loss: 0.5636314 Test Loss: 0.2340614\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\titers: 100, epoch: 22 | loss: 0.8059712\n",
      "\tspeed: 1.0176s/iter; left time: 36076.4353s\n",
      "\titers: 200, epoch: 22 | loss: 0.2580243\n",
      "\tspeed: 0.2632s/iter; left time: 9305.3066s\n",
      "\titers: 300, epoch: 22 | loss: 0.0636425\n",
      "\tspeed: 0.2691s/iter; left time: 9487.1697s\n",
      "\titers: 400, epoch: 22 | loss: 0.3904587\n",
      "\tspeed: 0.2665s/iter; left time: 9367.9991s\n",
      "Epoch: 22 cost time: 122.08536767959595\n",
      "Epoch: 22, Steps: 450 | Train Loss: 1.0008535 Vali Loss: 0.5636382 Test Loss: 0.2340655\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\titers: 100, epoch: 23 | loss: 0.8060001\n",
      "\tspeed: 0.9645s/iter; left time: 33758.1926s\n",
      "\titers: 200, epoch: 23 | loss: 0.2580549\n",
      "\tspeed: 0.2347s/iter; left time: 8189.8357s\n",
      "\titers: 300, epoch: 23 | loss: 0.0636552\n",
      "\tspeed: 0.2513s/iter; left time: 8746.8779s\n",
      "\titers: 400, epoch: 23 | loss: 0.3904676\n",
      "\tspeed: 0.2400s/iter; left time: 8328.2359s\n",
      "Epoch: 23 cost time: 110.48005509376526\n",
      "Epoch: 23, Steps: 450 | Train Loss: 1.0008618 Vali Loss: 0.5636326 Test Loss: 0.2340632\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\titers: 100, epoch: 24 | loss: 0.8059196\n",
      "\tspeed: 0.9700s/iter; left time: 33514.5016s\n",
      "\titers: 200, epoch: 24 | loss: 0.2580046\n",
      "\tspeed: 0.2378s/iter; left time: 8193.5188s\n",
      "\titers: 300, epoch: 24 | loss: 0.0636520\n",
      "\tspeed: 0.2462s/iter; left time: 8456.0045s\n",
      "\titers: 400, epoch: 24 | loss: 0.3904621\n",
      "\tspeed: 0.2482s/iter; left time: 8499.7355s\n",
      "Epoch: 24 cost time: 111.01187229156494\n",
      "Epoch: 24, Steps: 450 | Train Loss: 1.0008484 Vali Loss: 0.5636364 Test Loss: 0.2340686\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\titers: 100, epoch: 25 | loss: 0.8061265\n",
      "\tspeed: 0.9848s/iter; left time: 33584.0935s\n",
      "\titers: 200, epoch: 25 | loss: 0.2580536\n",
      "\tspeed: 0.2608s/iter; left time: 8867.4767s\n",
      "\titers: 300, epoch: 25 | loss: 0.0636546\n",
      "\tspeed: 0.2642s/iter; left time: 8956.5205s\n",
      "\titers: 400, epoch: 25 | loss: 0.3904588\n",
      "\tspeed: 0.2470s/iter; left time: 8348.9109s\n",
      "Epoch: 25 cost time: 116.68115520477295\n",
      "Epoch: 25, Steps: 450 | Train Loss: 1.0008533 Vali Loss: 0.5636274 Test Loss: 0.2340666\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\titers: 100, epoch: 26 | loss: 0.8060636\n",
      "\tspeed: 0.9532s/iter; left time: 32075.6667s\n",
      "\titers: 200, epoch: 26 | loss: 0.2579954\n",
      "\tspeed: 0.2488s/iter; left time: 8347.9401s\n",
      "\titers: 300, epoch: 26 | loss: 0.0636587\n",
      "\tspeed: 0.2428s/iter; left time: 8122.6797s\n",
      "\titers: 400, epoch: 26 | loss: 0.3904590\n",
      "\tspeed: 0.2402s/iter; left time: 8010.0556s\n",
      "Epoch: 26 cost time: 110.97697830200195\n",
      "Epoch: 26, Steps: 450 | Train Loss: 1.0008378 Vali Loss: 0.5636371 Test Loss: 0.2340712\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Long_Term_Forecast\n",
    "exp = Exp(config)  # set experiments\n",
    "\n",
    "for i in range(config.itr):\n",
    "    exp.train(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c21a3-0434-43e7-8bdb-98e286b6aeea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
