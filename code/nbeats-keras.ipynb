{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 02:34:15.584777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from kerasbeats import prep_time_series, NBeatsModel\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tscv\n",
    "\n",
    "def generate_label(df, threshold = 0.002):\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['target_15m'] <= -1*threshold), 'label'] = 1\n",
    "    df.loc[(df['target_15m'] >= threshold), 'label'] = 2\n",
    "    return df\n",
    "\n",
    "def get_na_features(df, train_features):\n",
    "    tmp = pd.DataFrame(df[train_features].isnull().sum())\n",
    "    tmp = tmp[tmp[0] > 0].reset_index()\n",
    "    tmp.columns = ['feat', 'cnt']\n",
    "    tmp = tmp.sort_values('cnt')\n",
    "    feat_groups = dict(tmp.groupby('cnt')['feat'].agg(lambda x: list(x)))\n",
    "    return feat_groups\n",
    "\n",
    "def normalize_float_columns(df, features):\n",
    "  float_cols = df[features].select_dtypes(include = [float]).columns\n",
    "  means = df[float_cols].mean().astype('float32')\n",
    "  std = df[float_cols].std().astype('float32')\n",
    "  df[float_cols] = df[float_cols].ffill().fillna(means)\n",
    "  df[float_cols] = (df[float_cols] - means) / std\n",
    "  return df, means, std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "window = 30\n",
    "file = f'target_{window}m_feature.csv'\n",
    "corr = pd.read_csv(f'../output/feature_corr/{file}', header = 0, index_col = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "df = pd.read_feather('../data/df_btc_with_features_5m_spot.feather')\n",
    "\n",
    "df['target_5m'] = df['close'].pct_change(1)\n",
    "df = df.dropna(subset = ['target_5m'], axis = 0)\n",
    "\n",
    "start_time = df['open_time'].min()\n",
    "end_time = df['open_time'].max()\n",
    "dates = df['open_time'].unique()\n",
    "n = len(dates)\n",
    "train_idx = int(0.7 * n)\n",
    "valid_idx = int(0.9 * n)\n",
    "train_end = dates[train_idx]\n",
    "valid_end = dates[valid_idx]\n",
    "\n",
    "train_df = df.loc[df['open_time'] < train_end].reset_index(drop=True)\n",
    "valid_df = df.loc[(train_end <= df['open_time']) & (df['open_time'] < valid_end)].reset_index(drop=True)\n",
    "\n",
    "test_df = df.loc[(df['open_time'] >= valid_end)].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "groups = pd.factorize(\n",
    "    train_df['open_time'].dt.day.astype(str) + '_' + train_df['open_time'].dt.month.astype(str) + '_' + train_df[\n",
    "        'open_time'].dt.year.astype(str))[0]\n",
    "\n",
    "cv = tscv.PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    group_gap=31,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "lookback = 10\n",
    "horizon = 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "directory = 'spot_data_11_25'\n",
    "date = '11_25'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.python.keras.backend as K\n",
    "import tensorflow.python.keras.layers as layers\n",
    "from tensorflow.python.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def create_nbeat_mlp(num_columns, num_labels, lookback, horizon, hidden_units, dropout_rates, ls=1e-2, lr=1e-3):\n",
    "    nbeats = NBeatsModel(model_type = 'generic', lookback = lookback, horizon = horizon,\n",
    "                         learning_rate = lr, batch_size = 4096,\n",
    "                         num_generic_neurons = hidden_units[0]) # set as default\n",
    "    nbeats.build_layer()\n",
    "    time_input = keras.layers.Input(shape = (lookback * horizon, ))\n",
    "    x_nb = nbeats.model_layer(time_input)\n",
    "\n",
    "    xcons = keras.layers.Input(shape = (num_columns, ))\n",
    "    x = keras.layers.Concatenate()([xcons, x_nb])\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "    for i in range(1, len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i])(x)\n",
    "\n",
    "    out = tf.keras.layers.Dense(num_labels, name = 'action')(x)\n",
    "    model = tf.keras.models.Model(inputs = [time_input, xcons], outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss = {'action' : tf.keras.losses.MeanSquaredError()},\n",
    "                  metrics = {'action' : tf.metrics.MeanSquaredError(name = 'mse')})\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(460235, 143)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "train_features_test = corr.iloc[:50].index.tolist()\n",
    "\n",
    "params = {'num_columns': len(train_features_test),\n",
    "          'num_labels': 1,\n",
    "          'lookback' : 10,\n",
    "          'horizon' : 6,\n",
    "          'hidden_units': [896, 448, 448, 256],\n",
    "          'dropout_rates': [0.42409238408801436, 0.10431484318345882,\n",
    "                            0.49230389137187497, 0.32024444956111164, 0.2716856145683449, 0.4379233941604448],\n",
    "          'ls': 0,\n",
    "          'lr': 1e-3,\n",
    "          }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_const is (68187, 50), x_tr is (68187, 60), y_tr is (68187, 6)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 28\u001B[0m\n\u001B[1;32m     22\u001B[0m ckp \u001B[38;5;241m=\u001B[39m ModelCheckpoint(ckp_path, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_action_mse\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     23\u001B[0m                               save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, save_weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     24\u001B[0m es \u001B[38;5;241m=\u001B[39m EarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_action_mse\u001B[39m\u001B[38;5;124m'\u001B[39m, min_delta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     25\u001B[0m                     baseline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 28\u001B[0m history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit([\u001B[43mx_tr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m, x_tr_const\u001B[38;5;241m.\u001B[39mvalues], y_tr\u001B[38;5;241m.\u001B[39mvalues,\n\u001B[1;32m     29\u001B[0m                     validation_data \u001B[38;5;241m=\u001B[39m ([x_val\u001B[38;5;241m.\u001B[39mvalues, x_val_const\u001B[38;5;241m.\u001B[39mvalues], y_val),\n\u001B[1;32m     30\u001B[0m                     epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m, batch_size \u001B[38;5;241m=\u001B[39m batch_size, callbacks \u001B[38;5;241m=\u001B[39m [ckp, es])\n\u001B[1;32m     32\u001B[0m hist \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(history\u001B[38;5;241m.\u001B[39mhistory)\n\u001B[1;32m     33\u001B[0m score \u001B[38;5;241m=\u001B[39m hist[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_action_mse\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmin()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "batch_size = 4096\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(train_df, train_df[f'target_5m'], groups)):\n",
    "    x_train, x_valid = train_df['target_5m'].iloc[train_idx], train_df['target_5m'].iloc[val_idx]\n",
    "\n",
    "    min_train, max_train = min(train_df['open_time'].iloc[train_idx]).to_pydatetime(), max(\n",
    "                train_df['open_time'].iloc[train_idx]).to_pydatetime()\n",
    "    min_valid, max_valid = min(train_df['open_time'].iloc[val_idx]).to_pydatetime(), max(\n",
    "                train_df['open_time'].iloc[val_idx]).to_pydatetime()\n",
    "\n",
    "    x_tr, y_tr = prep_time_series(x_train, lookback = lookback, horizon = horizon)\n",
    "    x_val, y_val = prep_time_series(x_valid, lookback = lookback, horizon = horizon)\n",
    "\n",
    "    cutoff_tr, cutoff_val = x_train.shape[0] - x_tr.shape[0], x_valid.shape[0] - x_val.shape[0]\n",
    "    x_tr_const, x_val_const = train_df[train_features_test].iloc[train_idx], train_df[train_features_test].iloc[val_idx]\n",
    "    x_tr_const, x_val_const = x_tr_const.iloc[cutoff_tr:, :], x_val_const.iloc[cutoff_val:, :]\n",
    "\n",
    "    print(f'Shape of X_const is {x_tr_const.shape}, x_tr is {x_tr.shape}, y_tr is {y_tr.shape}')\n",
    "\n",
    "    ckp_path = f'../output/{directory}/NBEATS_MSE_{fold}_returns{horizon}m_{lookback}m_{date}.hdf5'\n",
    "    model = create_nbeat_mlp(**params)\n",
    "    ckp = ModelCheckpoint(ckp_path, monitor='val_action_mse', verbose=0,\n",
    "                                  save_best_only=True, save_weights_only=True, mode='min')\n",
    "    es = EarlyStopping(monitor='val_action_mse', min_delta=1e-4, patience=10, mode='min',\n",
    "                        baseline=None, restore_best_weights=True, verbose=0)\n",
    "\n",
    "\n",
    "    history = model.fit([x_tr.values, x_tr_const.values], y_tr.values,\n",
    "                        validation_data = ([x_val.values, x_val_const.values], y_val),\n",
    "                        epochs = 100, batch_size = batch_size, callbacks = [ckp, es])\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    score = hist['val_action_mse'].min()\n",
    "    print(f'Fold {fold} MSE:\\t', score)\n",
    "    scores.append(score)\n",
    "    K.clear_session()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}